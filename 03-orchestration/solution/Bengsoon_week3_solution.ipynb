{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to familiarize users with workflow orchestration. We start from the solution of homework 1. The notebook can be found below:\n",
    "\n",
    "https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/01-intro/homework.ipynb\n",
    "\n",
    "This has already been converted to a script called `homework.py` in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "def read_data(path):\n",
      "    df = pd.read_parquet(path)\n",
      "    return df\n",
      "\n",
      "def prepare_features(df, categorical, train=True):\n",
      "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
      "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
      "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
      "\n",
      "    mean_duration = df.duration.mean()\n",
      "    if train:\n",
      "        print(f\"The mean duration of training is {mean_duration}\")\n",
      "    else:\n",
      "        print(f\"The mean duration of validation is {mean_duration}\")\n",
      "    \n",
      "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
      "    return df\n",
      "\n",
      "def train_model(df, categorical):\n",
      "\n",
      "    train_dicts = df[categorical].to_dict(orient='records')\n",
      "    dv = DictVectorizer()\n",
      "    X_train = dv.fit_transform(train_dicts) \n",
      "    y_train = df.duration.values\n",
      "\n",
      "    print(f\"The shape of X_train is {X_train.shape}\")\n",
      "    print(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
      "\n",
      "    lr = LinearRegression()\n",
      "    lr.fit(X_train, y_train)\n",
      "    y_pred = lr.predict(X_train)\n",
      "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
      "    print(f\"The MSE of training is: {mse}\")\n",
      "    return lr, dv\n",
      "\n",
      "def run_model(df, categorical, dv, lr):\n",
      "    val_dicts = df[categorical].to_dict(orient='records')\n",
      "    X_val = dv.transform(val_dicts) \n",
      "    y_pred = lr.predict(X_val)\n",
      "    y_val = df.duration.values\n",
      "\n",
      "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
      "    print(f\"The MSE of validation is: {mse}\")\n",
      "    return\n",
      "\n",
      "def main(train_path: str = './data/fhv_tripdata_2021-01.parquet', \n",
      "           val_path: str = './data/fhv_tripdata_2021-02.parquet'):\n",
      "\n",
      "    categorical = ['PUlocationID', 'DOlocationID']\n",
      "\n",
      "    df_train = read_data(train_path)\n",
      "    df_train_processed = prepare_features(df_train, categorical)\n",
      "\n",
      "    df_val = read_data(val_path)\n",
      "    df_val_processed = prepare_features(df_val, categorical, False)\n",
      "\n",
      "    # train the model\n",
      "    lr, dv = train_model(df_train_processed, categorical)\n",
      "    run_model(df_val_processed, categorical, dv, lr)\n",
      "\n",
      "main()\n"
     ]
    }
   ],
   "source": [
    "!cat homework.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation \n",
    "\n",
    "We already have a model training script. Maybe a data scientist in your team handed it to you and your job is schedule the running of training script using a workflow orchestration - Prefect in this case. Below are the requirements. Do not implement them yet, we will do so in this exercise. Just understand the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The training flow will be run every month.\n",
    "2. The flow will take in a parameter called `date` which will be a datetime.\n",
    "    * a. `date` should default to None\n",
    "    * b. If `date` is None, set `date` as the current day. Use the data from 2 months back as the training data and the data from the previous month as validation data.\n",
    "    * c. If `date` is passed, get 2 months before the `date` as the training data, and the previous month as validation data.\n",
    "    * d. As a concrete example, if the date passed is \"2021-03-15\", the training data should be \"fhv_tripdata_2021-01.parquet\" and the validation file will be \"fhv_trip_data_2021-02.parquet\"\n",
    "3. Save the model as \"model-{date}.pkl\" where date is in `YYYY-MM-DD`. Note that `date` here is the value of the flow `parameter`. In practice, this setup makes it very easy to get the latest model to run predictions because you just need to get the most recent one.\n",
    "4. In this example we use a DictVectorizer. That is needed to run future data through our model. Save that as \"dv-{date}.pkl\". Similar to above, if the date is `2021-03-15`, the files output should be `model-2021-03-15.bin` and `dv-2021-03-15.b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This convention is not strict in industry, and in practice, you will come up with your own system to manage these training pipeline runs. For example, if we wanted to train on the whole history instead of just one month, we'd need to allow for added parameterization and logic in our flow. If the data came in weekly instead of monthly, we might need a different naming convention. But these requirements are already a simple approximation of something you could use in production.\n",
    "\n",
    "On the deployment side, it's very easy to just pull in the latest data and predict it using the latest model and vectorizer files. Tools the MLFlow in the last chapter can simplify that process as well. This homework will focus more on the batch training.\n",
    "\n",
    "In order, this homework assignment will be about:\n",
    "\n",
    "1. Converting the script to a Flow\n",
    "2. Changing the parameters to take in a `date`. Making this parameter dynamic.\n",
    "3. Scheduling a batch training job that outputs the latest model somewhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Converting the script to a Prefect flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current script `homework.py` is a fully functional script as long as you already have `fhv_trip_data_2021-01.parquet` and `fhv_trip_data_2021-02.parquet` inside a `data` folder. You should be able to already run it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean duration of training is 16.2472533682457\n",
      "The mean duration of validation is 16.859265811074096\n",
      "The shape of X_train is (1109826, 525)\n",
      "The DictVectorizer has 525 features\n",
      "The MSE of training is: 10.528519395264997\n",
      "The MSE of validation is: 11.014287010952778\n"
     ]
    }
   ],
   "source": [
    "!python homework.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to bring this to workflow orchestration to add observability around it. The `main` function will be converted to a `flow` and the other functions will be `tasks`. After adding all of the decorators, there is actually one task that you will need to call `.result()` for inside the `flow` to get it to work. Which task is this?\n",
    "\n",
    "* `read_data`\n",
    "* `prepare_features`\n",
    "* ***`train_model`***\n",
    "* `run_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER***: `train_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Parameterizing the flow\n",
    "\n",
    "Right now there are two parameters for `main()` called `train_path` and `val_path`. We want to change the flow function to accept `date` instead. `date` should then be passed to a task that gives both the `train_path` and `val_path` to use.\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "```python\n",
    "@flow\n",
    "def main(date=None):\n",
    "    train_path, val_path = get_paths(date).result()\n",
    "    # rest of flow below\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The training flow will be run every month.\n",
    "2. The flow will take in a parameter called `date` which will be a datetime.\n",
    "    * a. `date` should default to None\n",
    "    * b. If `date` is None, set `date` as the current day. Use the data from 2 months back as the training data and the data from the previous month as validation data.\n",
    "    * c. If `date` is passed, get 2 months before the `date` as the training data, and the previous month as validation data.\n",
    "    * d. As a concrete example, if the date passed is \"2021-03-15\", the training data should be \"fhv_tripdata_2021-01.parquet\" and the validation file will be \"fhv_trip_data_2021-02.parquet\"\n",
    "3. Save the model as \"model-{date}.pkl\" where date is in `YYYY-MM-DD`. Note that `date` here is the value of the flow `parameter`. In practice, this setup makes it very easy to get the latest model to run predictions because you just need to get the most recent one.\n",
    "4. In this example we use a DictVectorizer. That is needed to run future data through our model. Save that as \"dv-{date}.pkl\". Similar to above, if the date is `2021-03-15`, the files output should be `model-2021-03-15.bin` and `dv-2021-03-15.b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/fhv_tripdata_2021-01.parquet', './data/fhv_tripdata_2021-02.parquet')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paths(date: str = None):\n",
    "    if date == None:\n",
    "        date = datetime.today()\n",
    "    else:\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    train_date = date - relativedelta(months=2)\n",
    "    val_date = date - relativedelta(months=1)\n",
    "\n",
    "    train_path = f\"./data/fhv_tripdata_{train_date.strftime('%Y')}-{train_date.strftime('%m')}.parquet\"\n",
    "    val_path = f\"./data/fhv_tripdata_{val_date.strftime('%Y')}-{val_date.strftime('%m')}.parquet\"\n",
    "\n",
    "    return train_path, val_path\n",
    "\n",
    "get_paths(\"2021-03-15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the relevant files needed to run the main flow if date is 2021-08-15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-11 01:19:57--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-06.parquet\n",
      "Connecting to 192.9.200.39:80... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 13208079 (13M) [binary/octet-stream]\n",
      "Saving to: ‘./data/fhv_tripdata_2021-06.parquet.1’\n",
      "\n",
      "fhv_tripdata_2021-0 100%[===================>]  12.60M   414KB/s    in 39s     \n",
      "\n",
      "2022-06-11 01:20:36 (335 KB/s) - ‘./data/fhv_tripdata_2021-06.parquet.1’ saved [13208079/13208079]\n",
      "\n",
      "--2022-06-11 01:20:37--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-07.parquet\n",
      "Connecting to 192.9.200.39:80... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 12650862 (12M) [binary/octet-stream]\n",
      "Saving to: ‘./data/fhv_tripdata_2021-07.parquet’\n",
      "\n",
      "fhv_tripdata_2021-0 100%[===================>]  12.06M  1.45MB/s    in 14s     \n",
      "\n",
      "2022-06-11 01:20:52 (893 KB/s) - ‘./data/fhv_tripdata_2021-07.parquet’ saved [12650862/12650862]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-06.parquet -P ./data\n",
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-07.parquet -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/fhv_tripdata_2021-06.parquet', './data/fhv_tripdata_2021-07.parquet')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paths(\"2021-08-15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "01:22:51.421 | INFO    | Task run 'run_model-6559300c-0' - The MSE of validation is: 11.637023826050765\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:*** 11.637"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Saving the model and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we are not saving the model and vectorizer for future use. You don't need a new task for this, you can just add it inside the `flow`. The requirements for filenames to save it as were mentioned in the Motivation section. They are pasted again here:\n",
    "\n",
    "* Save the model as \"model-{date}.pkl\" where date is in `YYYY-MM-DD`. Note that `date` here is the value of the flow `parameter`. In practice, this setup makes it very easy to get the latest model to run predictions because you just need to get the most recent one.\n",
    "* In this example we use a DictVectorizer. That is needed to run future data through our model. Save that as \"dv-{date}.pkl\". Similar to above, if the date is `2021-03-15`, the files output should be `model-2021-03-15.bin` and `dv-2021-03-15.b`.\n",
    "\n",
    "By using this file name, during inference, we can just pull the latest model from our model directory and apply it. Assuming we already had a list of filenames:\n",
    "\n",
    "```python\n",
    "['model-2021-03-15.bin', 'model-2021-04-15.bin', 'model-2021-05-15.bin']\n",
    "```\n",
    "\n",
    "We could do something like `sorted(model_list, reverse=False)[0]` to get the filename of the latest file. This is the simplest way to consistently use the latest trained model for inference. Tools like MLFlow give us more control logic to use flows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    # save the model\n",
      "    with open(f\"./artifacts/model-{date}.bin\", \"wb\") as f_out:\n",
      "        pickle.dump(lr, f_out)\n",
      "\n",
      "    # save the dictvectorizeR\n",
      "    with open(f\"./artifacts/dv-{date}.b\", \"wb\") as f_out:\n",
      "        pickle.dump(dv, f_out)\n",
      "\n",
      "    \n",
      "main(date=\"2021-08-15\")\n"
     ]
    }
   ],
   "source": [
    "!tail -10 homework.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxrwxrwx 1 bengsoon bengsoon   512 Jun 11 16:39 .\n",
      "drwxrwxrwx 1 bengsoon bengsoon   512 Jun 11 16:30 ..\n",
      "-rwxrwxrwx 1 bengsoon bengsoon 13191 Jun 11 16:39 dv-2021-08-15.b\n",
      "-rwxrwxrwx 1 bengsoon bengsoon  4581 Jun 11 16:39 model-2021-08-15.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -la ./artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is the file size of the `DictVectorizer` that we trained when the `date` is 2021-08-15?\n",
    "\n",
    "* 13,000 bytes \n",
    "* 23,000 bytes \n",
    "* 33,000 bytes \n",
    "* 43,000 bytes \n",
    "\n",
    "***ANSWER:*** 13,000 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Creating a deployment with a CronSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously showed the `IntervalSchedule` in the video tutorials. In some cases, the interval is too rigid. For example, what if we wanted to run this `flow` on the 15th of every month? An interval of 30 days would not be in sync. In cases like these, the `CronSchedule` is more appropriate. The documentation for that is [here](https://orion-docs.prefect.io/concepts/schedules/#cronschedule)\n",
    "\n",
    "Cron is an important part of workflow orchestration. It is used to schedule tasks, and was a predecessor for more mature orchestration frameworks. A lot of teams still use Cron in production. Even if you don't use Cron, the Cron expression is very common as a way to write a schedule, and the basics are worth learning for orchestration, even outside Prefect.\n",
    "\n",
    "For this exercise, use a `CronSchedule` when creating a Prefect deployment.\n",
    "\n",
    "Create a deployment with `prefect deployment create` after you write your `DeploymentSpec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the prefect storage first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                       Configured Storage                       \u001b[0m\n",
      "┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStorage Type\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStorage Version\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mServer Default \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "└────┴──────────────┴─────────────────┴──────┴─────────────────┘\n",
      "\u001b[2;3mNo default storage is set. Temporary local storage will be used.\u001b[0m\n",
      "\u001b[2;3m     Set a default with `prefect storage set-default <id>`      \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect storage ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a storage yet, so we will create a Local one (`~/.prefect`) through the terminal. Once it is done, we should see it again with `!prefect storage ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                               Configured Storage                               \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m                                      \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSto…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStor…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m                 \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                                  ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mType\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mVers…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mServer Default \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m9ec8d91f-10b1-419b-ad72-558925f03039\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mLoc…\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m1.0  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mloc…\u001b[0m\u001b[32m \u001b[0m│ ✅              │\n",
      "│\u001b[36m                                      \u001b[0m│\u001b[36m \u001b[0m\u001b[36mSto…\u001b[0m\u001b[36m \u001b[0m│\u001b[36m       \u001b[0m│\u001b[32m      \u001b[0m│                 │\n",
      "└──────────────────────────────────────┴──────┴───────┴──────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!prefect storage ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Cron expression to run a flow at 9 AM every 15th of the month?\n",
    "\n",
    "* `* * 15 9 0`\n",
    "* `9 15 * * *`\n",
    "* `0 9 15 * *`\n",
    "* `0 15 9 1 *`\n",
    "\n",
    "***ANSWER:*** `0 9 15 * *`\n",
    "\n",
    "![](./chron_schedule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Viewing the Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the deployment in the UI. When first loading, we may not see that many flows because the default filter is 1 day back and 1 day forward. Remove the filter for 1 day forward to see the scheduled runs. \n",
    "\n",
    "How many flow runs are scheduled by Prefect in advance? You should not be counting manually. There is a number of upcoming runs on the top right of the dashboard.\n",
    "\n",
    "* 0\n",
    "* 3\n",
    "* 10\n",
    "* 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./upcoming_runs.png)\n",
    "\n",
    "***ANSWER:*** 3 (or 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Creating a work-queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this flow, you will need an agent and a work queue. Because we scheduled our flow on every month, it won't really get picked up by an agent. For this exercise, create a work-queue from the UI and view it using the CLI. \n",
    "\n",
    "For all CLI commands with Prefect, you can use `--help` to get more information. \n",
    "\n",
    "For example,\n",
    "\n",
    "* `prefect --help`\n",
    "* `prefect work-queue --help`\n",
    "\n",
    "What is the command to view the available work-queues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: prefect work-queue [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Commands for work queue CRUD.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  clear-concurrency-limit  Clear any concurrency limits from a work queue.\n",
      "  create                   Create a work queue.\n",
      "  delete                   Delete a work queue by ID.\n",
      "  inspect                  Inspect a work queue by ID.\n",
      "  ls                       View all work queues.\n",
      "  pause                    Pause a work queue.\n",
      "  preview                  Preview a work queue.\n",
      "  resume                   Resume a paused work queue.\n",
      "  set-concurrency-limit    Set a concurrency limit on a work queue.\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `prefect work-queue inspect`\n",
    "    > - To inspect a work queue by ID\n",
    "* `prefect work-queue ls`\n",
    "    > - View all work queues\n",
    "* `prefect work-queue preview`\n",
    "    > - Preview a work queue (same as what Kevin did in the video)\n",
    "* `prefect work-queue list`\n",
    "    > - Invalid command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                             Work Queues                             \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                                  ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcurrency Limit\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m1e2f6c71-9495-4b81-8715-c4591dbd1a35\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mglobal\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34mNone\u001b[0m\u001b[34m             \u001b[0m\u001b[34m \u001b[0m│\n",
      "└──────────────────────────────────────┴────────┴───────────────────┘\n",
      "\u001b[31m                     (**) denotes a paused queue                     \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mScheduled Sta…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRun ID                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNa…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDeployment ID            \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-09-15 09…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m4b9aacca-b766-4bac-9bab-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32msp…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m41e12a25-cc02-4223-a042-…\u001b[0m\u001b[34m \u001b[0m│\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-08-15 09…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m875fe784-abd2-4944-b8a9-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mju…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m41e12a25-cc02-4223-a042-…\u001b[0m\u001b[34m \u001b[0m│\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-07-15 09…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mf0bc9751-7d5c-4774-8ec4-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mpo…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m41e12a25-cc02-4223-a042-…\u001b[0m\u001b[34m \u001b[0m│\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-06-15 09…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m1fb4e2d2-88fc-489f-bacc-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mpe…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m41e12a25-cc02-4223-a042-…\u001b[0m\u001b[34m \u001b[0m│\n",
      "└────────────────┴───────────────────────────┴─────┴───────────────────────────┘\n",
      "\u001b[31m                            (**) denotes a late run                             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue preview 1e2f6c71-9495-4b81-8715-c4591dbd1a35 --hours 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mWorkQueue\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mid\u001b[0m=\u001b[32m'1e2f6c71-9495-4b81-8715-c4591dbd1a35'\u001b[0m,\n",
      "    \u001b[33mcreated\u001b[0m=\u001b[32m'1 day ago'\u001b[0m,\n",
      "    \u001b[33mupdated\u001b[0m=\u001b[32m'1 day ago'\u001b[0m,\n",
      "    \u001b[33mname\u001b[0m=\u001b[32m'global'\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue  1e2f6c71-9495-4b81-8715-c4591dbd1a35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:*** `prefect work-queue ls`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67953b799875afe0c9d1480a128619b95a76c682134b0fd77e7a7dc47c06c4d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlops_orchestration')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
